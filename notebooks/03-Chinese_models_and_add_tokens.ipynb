{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore some Chinese transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LTP chinese tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ltp  ## for chinese tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "LTP_RESOURCE='../data/pre_trained_model/ltp/base/'  \n",
    "#download from here https://pypi.org/project/ltp/\n",
    "#https://github.com/HIT-SCIR/ltp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu/anaconda3/envs/pytorch_transformer/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py:370: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  else:\n"
     ]
    }
   ],
   "source": [
    "ltp = LTP(LTP_RESOURCE) # 默认加载 Small 模型\n",
    "seg, hidden = ltp.seg([\"他叫汤姆去拿外衣。\"])\n",
    "pos = ltp.pos(hidden)\n",
    "ner = ltp.ner(hidden)\n",
    "srl = ltp.srl(hidden)\n",
    "dep = ltp.dep(hidden)\n",
    "sdp = ltp.sdp(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word segments: [['他', '叫', '汤姆', '去', '拿', '外衣', '。']]\n",
      "pos tags: [['r', 'v', 'nh', 'v', 'v', 'n', 'wp']]\n",
      "NER: [[('Nh', 2, 2)]]\n",
      "Dependency: [[(1, 2, 'SBV'), (2, 0, 'HED'), (3, 2, 'DBL'), (4, 2, 'VOB'), (5, 4, 'COO'), (6, 5, 'VOB'), (7, 2, 'WP')]]\n"
     ]
    }
   ],
   "source": [
    "print('word segments: {}'.format(seg))\n",
    "print('pos tags: {}'.format(pos))\n",
    "print('NER: {}'.format(ner))\n",
    "print('Dependency: {}'.format(dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some community pretraiend chinese models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8851206710>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer,BertModel\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load Chinese BERT and other models trained by community \n",
    "# https://huggingface.co/hfl\n",
    "# https://github.com/ymcui/Chinese-BERT-wwm\n",
    "cn_bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "cn_tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single segment token      : [['[CLS]', 'hu', '##gg', '##ing', 'face', '是', '一', '个', '不', '错', '的', 'pack', '##age', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '这', '是', '第', '二', '个', '测试', '句子', '~', '~', '!', '[UNK]', '[UNK]', 'fa', '##sd', '##f', '##q', '##pr', '##ul', '##z', ';', 'n', '[SEP]']]\n",
      "Single segment token      : tensor([[  101, 12199,  9949,  8221, 10656,  3221,   671,   702,   679,  7231,\n",
      "          4638, 12736,  9103,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  6821,  3221,  5018,   753,   702, 21128, 21129,   172,   172,\n",
      "           106,   100,   100, 12289, 10117,  8189,  8326, 11426, 10086,  8253,\n",
      "           132,   156,   102]])\n",
      "Single segment type       : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Single segment type       : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cn_input = cn_tokenizer([\"Hugging Face是一个不错的package\",\n",
    "                        \"这是第二个测试句子~~!``fasdfqprulz;n\"],\n",
    "                        add_special_tokens=True, truncation=True, max_length=512, padding=True ) \n",
    "## for some reason return_tensor = \"pt\" does not work\n",
    "def convert_to_pt(input_dict):\n",
    "    out = {k:torch.tensor(v) for k,v in input_dict.items()}\n",
    "    return out \n",
    "cn_input = convert_to_pt(cn_input)\n",
    "print(\"Single segment token      : {}\".format(\n",
    "    [cn_tokenizer.convert_ids_to_tokens(i) for i in cn_input['input_ids']]\n",
    "    ))\n",
    "      \n",
    "print(\"Single segment token      : {}\".format(cn_input['input_ids']))\n",
    "print(\"Single segment type       : {}\".format(cn_input['token_type_ids']))\n",
    "print(\"Single segment type       : {}\".format(cn_input['attention_mask']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token wise output: torch.Size([2, 25, 1024]), Pooled output: torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "output, pooled = cn_bert(**cn_input)\n",
    "\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(output.shape, pooled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- look at documentation here \n",
    "https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=add_token#transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length of tokenizer: 21128\n",
      "We have added 4 tokens\n",
      "after adding, length of tokenizer: 21132\n"
     ]
    }
   ],
   "source": [
    "## load tokenizer and add new vocabulary\n",
    "cn_tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "print('Original length of tokenizer: {}'.format(len(cn_tokenizer)))\n",
    "num_added_toks = cn_tokenizer.add_tokens(new_tokens=['测试','句子','随便一个','什么东西'],special_tokens=False)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "print('after adding, length of tokenizer: {}'.format(len(cn_tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 1024)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# afterward we need to resize our model to reflect that \n",
    "# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "cn_bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the same example with update tokenizer and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single segment token      : [['[CLS]', 'hu', '##gg', '##ing', 'face', '是', '一', '个', '不', '错', '的', 'pack', '##age', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '这', '是', '第', '二', '个', '测试', '句子', '~', '~', '!', '[UNK]', '[UNK]', 'fa', '##sd', '##f', '##q', '##pr', '##ul', '##z', ';', 'n', '[SEP]']]\n",
      "Single segment token      : tensor([[  101, 12199,  9949,  8221, 10656,  3221,   671,   702,   679,  7231,\n",
      "          4638, 12736,  9103,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  6821,  3221,  5018,   753,   702, 21128, 21129,   172,   172,\n",
      "           106,   100,   100, 12289, 10117,  8189,  8326, 11426, 10086,  8253,\n",
      "           132,   156,   102]])\n",
      "Single segment type       : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Single segment type       : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cn_input = cn_tokenizer([\"Hugging Face是一个不错的package\",\n",
    "                        \"这是第二个测试句子~~!``fasdfqprulz;n\"],\n",
    "                        add_special_tokens=True, truncation=True, max_length=512, padding=True ) \n",
    "## for some reason return_tensor = \"pt\" does not work\n",
    "def convert_to_pt(input_dict):\n",
    "    out = {k:torch.tensor(v) for k,v in input_dict.items()}\n",
    "    return out \n",
    "cn_input = convert_to_pt(cn_input)\n",
    "print(\"Single segment token      : {}\".format(\n",
    "    [cn_tokenizer.convert_ids_to_tokens(i) for i in cn_input['input_ids']]\n",
    "    ))\n",
    "      \n",
    "print(\"Single segment token      : {}\".format(cn_input['input_ids']))\n",
    "print(\"Single segment type       : {}\".format(cn_input['token_type_ids']))\n",
    "print(\"Single segment type       : {}\".format(cn_input['attention_mask']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cn_tokenizer.save_pretrained('.')\n",
    "#cn_tokenizer.from_pretrained('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
